trigger:
- master

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: UsePythonVersion@0
  displayName: 'Use Python 3.x'

- bash: |
   # Install Python packages
   
   python -m pip install --upgrade pip 
   python -m pip install -r requirements.txt
  failOnStderr: 'true'
  displayName: 'Install Packages'

- script: |
   pip install flake8
   pip install flake8_formatter_junit_xml
   flake8 --format junit-xml --output-file $(Build.BinariesDirectory)/flake8_report.xml --ignore=E111,E402
  displayName: 'Check code quality'
  
- task: PublishTestResults@2
  condition: succeededOrFailed()
  inputs:
    testResultsFiles: '$(Build.BinariesDirectory)/*_report.xml'
    testRunTitle: 'Publish test results'

- bash: |
   #Create Azure Databricks cluster
   
   DATABRICKS_CLUSTER_ID=$(python3 $(Build.SourcesDirectory)/cluster_config/cluster_manager.py 2>&1 >/dev/null) 
   
   echo "##vso[task.setvariable variable=existing_databricks_cluster_id]$DATABRICKS_CLUSTER_ID"  
  env:
    DATABRICKS_ACCESS_TOKEN: $(DATABRICKS_ACCESS_TOKEN)
  failOnStderr: 'true'
  displayName: 'Initialize Databricks Cluster'

- bash: |
   # Login to Databricks CLI and create dbfs direcotory for model
   databricks configure --token << ANSWERS
   https://$(DATABRICKS_DOMAIN)
   $(DATABRICKS_ACCESS_TOKEN)
   ANSWERS
   dbfs mkdirs dbfs:/model
  failOnStderr: 'true'
  displayName: 'Login to ADB CLI and create DBFS model directory'

- bash: |
   # Invoke the Python training pipeline
      
   python3 $(Build.SourcesDirectory)/aml_service/pipelines/train_pipeline.py
  failOnStderr: 'false'
  env:
    SOURCES_DIR: '$(Build.SourcesDirectory)'
    SP_APP_SECRET: '$(SP_APP_SECRET)'
    DATABRICKS_ACCESS_TOKEN: '$(DATABRICKS_ACCESS_TOKEN)'
  displayName: 'Train model using AML with Remote Compute'
  enabled: 'true'

- bash: |
   # Clean dbfs model directory
   dbfs rm dbfs:/model -r
  failOnStderr: 'true'
  displayName: 'Remove DBFS model directory'

- bash: |
   # Terminate Azure Databricks cluster
   
   python3 $(Build.SourcesDirectory)/cluster_config/cluster_manager.py --terminate
   
   echo "Cluster: $(existing_databricks_cluster_id) terminated successfully."
  failOnStderr: 'true'
  displayName: 'Terminate Databricks Cluster'
  enabled: 'false'
